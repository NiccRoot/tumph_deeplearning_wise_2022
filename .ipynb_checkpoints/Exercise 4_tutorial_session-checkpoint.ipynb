{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b8d52ba",
   "metadata": {},
   "source": [
    "## Building a Multilayer Perceptron\n",
    "\n",
    "Let's follow the lecture and build a multilayer perceptron to approximate the function $\\theta(x)$ in our variational inference model $$\\mathrm{Bernoulli}(z|\\theta(x))$$ with the following structure\n",
    "\n",
    "* 2 input nodes\n",
    "* 2 internal features computed using ReLu activation\n",
    "* 1 output feature with sigmoid activation that \n",
    "\n",
    "From Ex3, we had already the following code ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ab623e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def linear(X,pars):\n",
    "    out = X @ pars[:2].T + pars[2]\n",
    "    grad_pars = np.column_stack([X,np.ones(len(X))])\n",
    "    return out,grad_pars\n",
    "\n",
    "def sigmoid(x):\n",
    "    out = 1/(1+np.exp(-x))\n",
    "    grad = out*(1-out)\n",
    "    return out,grad.reshape(-1,1)\n",
    "\n",
    "def theta_perceptron(X,pars):\n",
    "    feature1,grad_linear1 = linear(X,pars[0:3])\n",
    "    activation1,grad_sigmoid1 = sigmoid(feature1)\n",
    "    return activation1, grad_sigmoid1*grad_linear1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce90f49a",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "* Write a function `relu(x)` that produces the relu activation function and its gradient.\n",
    "* make sure the gradient is returned as a shape (N,1)\n",
    "* plot it for the range: `x = np.linspace(-5,5,1001)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db1a097c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ReLU(x):\n",
    "    x=np.where(x>0,x,np.zeros_like(x))\n",
    "    grad=np.where(x>0,np.ones_like(x),np.zeros_like(x))\n",
    "    return x,grad\n",
    "x = np.linspace(-5,5,1001)\n",
    "y1,g1=ReLU(x)\n",
    "plt.plot(y1)\n",
    "plt.plot(g1)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c484d",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Write a function `multilayer_perceptron(X,pars)` that takes 9 parameters (3 for each artificial neurons: 2 weights and 1 bias) that can compute the $\\theta(x)$ prediction for many input points at once (batched computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6890e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(X,pars):\n",
    "    f1,gradf1 = linear(X,pars[:3])\n",
    "    f2,gradf2 = linear(X,pars[3:6])\n",
    "    Rf1,Rgradf1 = ReLU(f1)\n",
    "    Rf2,Rgradf2 = ReLU(f2)\n",
    "    Rf = np.column_stack([Rf1,Rf2])\n",
    "    F,gradF = linear(Rf,pars[6:])\n",
    "    out,gradout = sigmoid(out)\n",
    "    return out "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c370d21",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "\n",
    "Use this function to plot the contour of the multilayer perceptron\n",
    "\n",
    "```python\n",
    "def plot_contour(func,pars):\n",
    "    grid = np.mgrid[-5:5:101j,-5:5:101j]\n",
    "    X = np.swapaxes(grid,0,-1).reshape(-1,2)\n",
    "    out = func(X,pars)\n",
    "    out = np.swapaxes(out.reshape(101,101),0,-1)\n",
    "    plt.contour(grid[0],grid[1],out)\n",
    "```\n",
    "\n",
    "Try for example parameter vectors: `np.array([1,0,0,0,1,0,1,1,0])` or what every you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0c6c2e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mswapaxes(out\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m101\u001b[39m,\u001b[38;5;241m101\u001b[39m),\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39mcontour(grid[\u001b[38;5;241m0\u001b[39m],grid[\u001b[38;5;241m1\u001b[39m],out)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mplot_contour\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmultilayer_perceptron\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mplot_contour\u001b[1;34m(func, pars)\u001b[0m\n\u001b[0;32m      2\u001b[0m grid \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmgrid[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m:\u001b[38;5;241m5\u001b[39m:\u001b[38;5;241m101\u001b[39mj,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m:\u001b[38;5;241m5\u001b[39m:\u001b[38;5;241m101\u001b[39mj]\n\u001b[0;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mswapaxes(grid,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mswapaxes(out\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m101\u001b[39m,\u001b[38;5;241m101\u001b[39m),\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mcontour(grid[\u001b[38;5;241m0\u001b[39m],grid[\u001b[38;5;241m1\u001b[39m],out)\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mmultilayer_perceptron\u001b[1;34m(X, pars)\u001b[0m\n\u001b[0;32m      6\u001b[0m Rf \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcolumn_stack([Rf1,Rf2])\n\u001b[0;32m      7\u001b[0m F,gradF \u001b[38;5;241m=\u001b[39m linear(Rf,pars[\u001b[38;5;241m6\u001b[39m:])\n\u001b[1;32m----> 8\u001b[0m outt,gradout \u001b[38;5;241m=\u001b[39m sigmoid(\u001b[43mout\u001b[49m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outt\n",
      "\u001b[1;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_contour(func,pars):\n",
    "    grid = np.mgrid[-5:5:101j,-5:5:101j]\n",
    "    X = np.swapaxes(grid,0,-1).reshape(-1,2)\n",
    "    out = func(X,pars)\n",
    "    out = np.swapaxes(out.reshape(101,101),0,-1)\n",
    "    plt.contour(grid[0],grid[1],out)\n",
    "    \n",
    "plot_contour(multilayer_perceptron,np.array([1,0,0,0,1,0,1,1,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee208894",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "\n",
    "Now comes the hard part!\n",
    "\n",
    "We want to compute gradients for this function\n",
    "\n",
    "$$\n",
    "\\vec{a} = [\\;\\mathrm{ReLU}(\\mathrm{Lin}(\\vec{x},\\phi_1))\\;,\\;\\mathrm{ReLU}(\\mathrm{Lin}(\\vec{x},\\phi_2))\\;]\\\\\n",
    "a_2 = \\sigma(\\mathrm{Lin}(\\vec{a},\\phi_3))\\\\\n",
    "$$\n",
    "\n",
    "To compute the gradient we need to also have the gradients $\\frac{\\partial \\mathrm{Lin}}{\\partial \\vec{x}}$\n",
    "\n",
    "* Write a new function that also outputs the partial derivatives with respect to $\\vec{x}$\n",
    "* Hint: the output shape of `grad_x` should be (1,3)!\n",
    "\n",
    "```python\n",
    "def linear(X,pars):\n",
    "   ...\n",
    "   return out,grad_pars,grad_x\n",
    "```\n",
    "$\n",
    "f1=L_1(x,\\Phi)\\\\\n",
    "a1=R1(f1)\\\\\n",
    "m=g(A,\\Phi)=g(a1,a2,\\Phi)\\\\\n",
    "o=\\sigma(m)\\\\\n",
    "\\partial_m\\sigma\\cdot\\partial_{a_1}m\\cdot\\partial_{f_1}a_1\\cdot\\partial_{\\Phi}f_1\\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245c6b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(X,pars):\n",
    "    out = X @ pars[:2] + pars[2]\n",
    "    grad_pars = np.column_stack([X,np.ones(len(X))])\n",
    "    grad_X = pars[:2]\n",
    "    return out,grad_pars,grad_x.reshape(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9180f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 6\n",
    "0.0299\n",
    "With this in hadn you can now carefully piece back the gradients together.\n",
    "\n",
    "We can start with the function like this\n",
    "\n",
    "```python\n",
    "def multilayer_perceptron(X,pars):\n",
    "    feature1,grad_f1,_ = linear(X,pars[0:3])\n",
    "    feature2,grad_f2,_ = linear(X,pars[3:6])\n",
    "    activation1,grad_r1 = relu(feature1)\n",
    "    activation2,grad_r2 = relu(feature2)\n",
    "    hidden = np.column_stack([activation1,activation2])\n",
    "    output_feature,grad_f3,grad_x = linear(hidden,pars[6:9]) #here is the new gradient!\n",
    "    theta,grad_sig = sigmoid(output_feature)\n",
    "    return theta\n",
    "```\n",
    "\n",
    "* Try to work out what the 9-dimensional gradient vector looks like for $\\nabla_\\phi \\theta(x,\\phi)$\n",
    "* Hint 1: the gradient should have the shape `(N,9)`\n",
    "* Hint 2: The following would be a correct result\n",
    "\n",
    "```python\n",
    "Xtest = np.array([[1.0,2.0]])\n",
    "pars = np.array([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])\n",
    "multilayer_perceptron(Xtest,pars)\n",
    "\n",
    "value: [0.9552123]\n",
    "gradient: [[0.02994724 0.05989447 0.02994724 0.03422541 0.06845082 0.03422541\n",
    "  0.03422541 0.08556353 0.04278176]]\n",
    "``` \n",
    "\n",
    "### Step 7\n",
    "\n",
    "## Training the Multi-Layer Perceptron\n",
    "\n",
    "We can adapt the learning code from Exercise 3 to learn the multi-layer perceptron.\n",
    "\n",
    "We can get some interesting datasets from the `scikit-learn` package\n",
    "\n",
    "* Install scikit-learn via `pip install scikit-learn`\n",
    "* Use the following data-generating function and plot the data\n",
    "\n",
    "```python\n",
    "def generate_data(N):\n",
    "    import sklearn.datasets as skld\n",
    "    X,z = skld.make_circles(N, noise = 0.1, factor=0.5)\n",
    "    filt =  (X[:,1] > 0)\n",
    "    return X[filt],z[filt]\n",
    "```\n",
    "\n",
    "We can now adapt our training code from Exercise 3 (take some time to go through it, but nothing fundamental changed) and train out multilayer perceptron!\n",
    "\n",
    "```python\n",
    "def loss(z,theta):\n",
    "    out = np.where(z==1,-np.log(theta),-np.log(1-theta))\n",
    "    grad = np.where(z==1,-1/theta,-1/(1-theta)*(-1))\n",
    "    return out,grad.reshape(-1,1)\n",
    "\n",
    "def empirical_risk(X,z,theta_func,pars):\n",
    "    theta,grad_theta = theta_func(X,pars)\n",
    "    loss_val,grad_loss = loss(z,theta)\n",
    "    grad1 = grad_loss*grad_theta\n",
    "    grad = np.concatenate([grad1], axis=-1)\n",
    "    return loss_val.mean(axis=0),grad.mean(axis=0),theta\n",
    "\n",
    "def plot(X,z,theta_func,pars):\n",
    "    grid = np.mgrid[-5:5:101j,-5:5:101j]\n",
    "    Xi = np.swapaxes(grid,0,-1).reshape(-1,2)   \n",
    "    _,_,zi = empirical_risk(Xi,np.zeros(len(Xi)),theta_func,pars)\n",
    "    zi = zi.reshape(101,101).T\n",
    "    plt.contour(grid[0],grid[1],zi, levels = np.linspace(0,1,21))\n",
    "    plt.scatter(X[:,0],X[:,1],c = z)\n",
    "    plt.xlim(-2,2)\n",
    "    plt.ylim(-2,2)\n",
    "\n",
    "def learn(data,pars,theta_func, nsteps = 15000):\n",
    "    X,z = data\n",
    "    for i in range(nsteps):\n",
    "        val,grad,_ = empirical_risk(X,z,theta_func,pars)\n",
    "        pars = pars - 0.01*grad\n",
    "        if i % (nsteps//4) == 0:\n",
    "            print(val,pars)\n",
    "            plot(X,z,theta_func,pars)\n",
    "            plt.gcf().set_size_inches(3,3)\n",
    "            plt.show()\n",
    "    return pars\n",
    "```\n",
    "\n",
    "# Try Learning\n",
    "\n",
    "Try learning on a dataset of 1000 samples and initialize the parameters with \n",
    "\n",
    "`pars = np.array([-.1,1,0,.1,1,0,-.1,-.1,0])`\n",
    "\n",
    "* The learning itself, depending on the initialization might or might not be super-convincing\n",
    "* Try executing this multiple times to try out different initializations\n",
    "* Ultimately, we will need to add more & more neurons, but as you see the gradient calculation is painful!\n",
    "* Try other initializations and see what happens\n",
    "\n",
    "### Going Beyond\n",
    "\n",
    "* Can you extend this to N hidden neurons?\n",
    "* If yes, try the following learning problem with e.g. 15 neurons\n",
    "\n",
    "```python\n",
    "def generate_data(N):\n",
    "    import sklearn.datasets as skld\n",
    "    X,z = skld.make_circles(N, noise = 0.1, factor=0.5)\n",
    "    return X,z\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
